"""LLM Text Generation Microservice.

A FastAPI-based microservice for text generation using Groq LLM.
Includes monitoring, tracing, and service discovery capabilities.
"""

# Standard library imports
from datetime import UTC, datetime
import logging
import os
import time
from typing import Dict, Optional

# Third-party imports
import consul
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import groq
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc import (
    trace_exporter
)
from opentelemetry.instrumentation import fastapi
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace import export
from prometheus_client import Counter, Histogram, generate_latest
from pydantic import BaseModel
from tenacity import retry, stop_after_attempt, wait_exponential
import uvicorn

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

# Load environment variables from a .env file
load_dotenv()

# Initialize FastAPI app with metadata
app = FastAPI(
    title="LLM Text Generation Microservice",
    description="A microservice for text generation with Groq LLM",
    version="1.0.0",
)

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
)

# Initialize Prometheus metrics for monitoring
REQUEST_COUNT = Counter(
    "request_count",
    "Total number of requests",
)
GENERATION_TIME = Histogram(
    "generation_time_seconds",
    "Time spent generating text",
)
ERROR_COUNT = Counter(
    "error_count",
    "Total number of errors",
)

# Initialize OpenTelemetry for distributed tracing
tracer_provider = TracerProvider()
try:
    # Attempt to use OTLP exporter for sending traces
    otlp_exporter = trace_exporter.OTLPSpanExporter(
        endpoint=os.getenv(
            "OTEL_EXPORTER_OTLP_ENDPOINT",
            "http://localhost:4317",
        )
    )
    span_processor = export.BatchSpanProcessor(otlp_exporter)
except Exception as e:
    # Fallback to console exporter if OTLP fails
    logging.error(
        "Failed to initialize OTLP exporter, falling back to console: %s",
        e,
    )
    span_processor = export.BatchSpanProcessor(export.ConsoleSpanExporter())

# Set the tracer provider and instrument the FastAPI app
tracer_provider.add_span_processor(span_processor)
trace.set_tracer_provider(tracer_provider)
fastapi.FastAPIInstrumentor.instrument_app(app)

# Initialize Consul client for service discovery
consul_client = consul.Consul(
    host=os.getenv("CONSUL_HOST", "localhost"),
    port=int(os.getenv("CONSUL_PORT", 8500)),
)

# Initialize Groq client for LLM interactions
try:
    groq_client = groq.Groq(api_key=os.getenv("GROQ_API_KEY"))
except Exception as e:
    logging.error("Error initializing Groq client: %s", e)
    groq_client = None


class TextGenerationRequest(BaseModel):
    """Request model for text generation endpoint.

    Attributes:
        prompt: The input text to generate from.
        max_tokens: Maximum number of tokens to generate.
        temperature: Sampling temperature for generation.
    """

    prompt: str
    max_tokens: Optional[int] = 150
    temperature: Optional[float] = 0.7


class TextGenerationResponse(BaseModel):
    """Response model for text generation endpoint.

    Attributes:
        generated_text: The text generated by the LLM.
        model: The name of the LLM model used.
        usage: Token usage statistics.
    """

    generated_text: str
    model: str
    usage: Dict[str, int]


class HealthCheckResponse(BaseModel):
    """Response model for health check endpoint.

    Attributes:
        status: Overall service health status.
        service: Name of the service.
        timestamp: Current timestamp.
        model_status: LLM model health status.
        consul_status: Consul connection status.
        metrics_status: Prometheus metrics status.
    """

    status: str
    service: str
    timestamp: str
    model_status: str
    consul_status: str
    metrics_status: str


@app.get("/")
async def root():
    """Return basic service health status.

    Returns:
        dict: Basic health status information.
    """
    logging.info("Health check endpoint accessed")
    return {
        "status": "healthy",
        "service": "LLM Text Generation Microservice",
    }


@app.get("/health", response_model=HealthCheckResponse)
async def health_check():
    """Check health of all service components.

    Returns:
        HealthCheckResponse: Detailed health status of all components.
    """
    logging.info("Health check endpoint accessed")
    model_status = "healthy" if groq_client else "unhealthy"
    consul_status = "healthy" if consul_client else "unhealthy"
    metrics_status = "healthy" if generate_latest() else "unhealthy"

    return HealthCheckResponse(
        status="healthy",
        service="LLM Text Generation Microservice",
        timestamp=datetime.now(UTC).isoformat(),
        model_status=model_status,
        consul_status=consul_status,
        metrics_status=metrics_status,
    )


@app.get("/metrics")
async def metrics():
    """Expose Prometheus metrics.

    Returns:
        bytes: Prometheus metrics in text format.
    """
    logging.info("Metrics endpoint accessed")
    return generate_latest()


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
)
async def generate_with_retry(
    prompt: str,
    max_tokens: int,
    temperature: float,
):
    """Generate text with retry logic using circuit breaker pattern.

    Args:
        prompt: The input text to generate from.
        max_tokens: Maximum number of tokens to generate.
        temperature: Sampling temperature for generation.

    Returns:
        The generated text response from the LLM.

    Raises:
        Exception: If text generation fails after retries.
    """
    try:
        logging.info("Attempting to generate text with retry logic")
        response = groq_client.chat.completions.create(
            model="gemma2-9b-it",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return response
    except Exception as e:
        ERROR_COUNT.inc()
        logging.error("Error during text generation: %s", e)
        raise


@app.post("/generate", response_model=TextGenerationResponse)
async def generate_text(request: TextGenerationRequest):
    """Generate text based on the provided prompt using Groq LLM.

    Args:
        request: TextGenerationRequest containing the prompt and parameters.

    Returns:
        TextGenerationResponse containing the generated text and metadata.

    Raises:
        HTTPException: If the model is not initialized or generation fails.
    """
    logging.info("Generate text endpoint accessed")
    if groq_client is None:
        logging.error("Groq client not initialized")
        raise HTTPException(
            status_code=503,
            detail="Model service not initialized. Please try again later.",
        )

    REQUEST_COUNT.inc()

    try:
        with GENERATION_TIME.time():
            start_time = time.time()

            # Generate text using Groq with retry logic
            response = await generate_with_retry(
                request.prompt,
                request.max_tokens,
                request.temperature,
            )

            processing_time = time.time() - start_time
            logging.info(
                "Text generation completed in %.2f seconds",
                processing_time,
            )

            # Register service with Consul
            try:
                consul_client.agent.service.register(
                    "llm-service",
                    service_id="llm-service-1",
                    port=8000,
                    check={
                        "http": "http://localhost:8000/health",
                        "interval": "10s",
                    },
                )
            except Exception as e:
                logging.error("Failed to register service with Consul: %s", e)

            return TextGenerationResponse(
                generated_text=response.choices[0].message.content,
                model=response.model,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
            )

    except Exception as e:
        ERROR_COUNT.inc()
        logging.error("Text generation failed: %s", e)
        raise HTTPException(
            status_code=500,
            detail=f"Text generation failed: {str(e)}",
        )


if __name__ == "__main__":
    # Register service with Consul on startup
    try:
        consul_client.agent.service.register(
            "llm-service",
            service_id="llm-service-1",
            port=8000,
            check={
                "http": "http://localhost:8000/health",
                "interval": "10s",
            },
        )
    except Exception as e:
        logging.error("Failed to register with Consul: %s", e)

    # Run the FastAPI app with Uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
